"""
Performance benchmarks for neuro-symbolic law prover.
"""

import pytest
import time
import asyncio
import statistics
import psutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any
import gc
import json
from pathlib import Path

from neuro_symbolic_law.core.legal_prover import LegalProver
from neuro_symbolic_law.core.enhanced_prover import EnhancedLegalProver
from neuro_symbolic_law.parsing.contract_parser import ContractParser
from neuro_symbolic_law.regulations.gdpr import GDPR
from neuro_symbolic_law.regulations.ai_act import AIAct
from neuro_symbolic_law.regulations.ccpa import CCPA
from neuro_symbolic_law.performance.cache_manager import CacheManager
from neuro_symbolic_law.performance.resource_pool import WorkerPool


@pytest.mark.performance
class TestPerformanceBenchmarks:
    """Performance benchmark tests."""
    
    @pytest.fixture(autouse=True)
    def setup_benchmark(self):
        """Setup for benchmark tests."""
        self.process = psutil.Process()
        self.start_memory = self.process.memory_info().rss
        self.start_cpu_percent = self.process.cpu_percent()
        
        yield
        
        # Cleanup after each test
        gc.collect()
    
    def test_single_contract_verification_speed(self, sample_contract_text):
        """Benchmark single contract verification speed."""
        parser = ContractParser()
        prover = LegalProver()
        regulation = GDPR()
        
        # Parse contract
        contract = parser.parse(sample_contract_text, "benchmark_contract")
        
        # Warm-up run
        prover.verify_compliance(contract, regulation)
        
        # Benchmark runs
        times = []
        for i in range(10):
            start_time = time.perf_counter()
            results = prover.verify_compliance(contract, regulation)
            end_time = time.perf_counter()
            
            times.append(end_time - start_time)
            assert len(results) > 0, "Should produce verification results"
        
        # Performance assertions
        avg_time = statistics.mean(times)\n        median_time = statistics.median(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0
        
        print(f\"\\nSingle Contract Verification Performance:\")\n        print(f\"Average time: {avg_time:.3f}s\")\n        print(f\"Median time: {median_time:.3f}s\")\n        print(f\"Standard deviation: {std_dev:.3f}s\")\n        print(f\"Min time: {min(times):.3f}s\")\n        print(f\"Max time: {max(times):.3f}s\")\n        \n        # Performance targets\n        assert avg_time < 2.0, f\"Average verification time {avg_time:.3f}s exceeds 2.0s target\"\n        assert median_time < 1.5, f\"Median verification time {median_time:.3f}s exceeds 1.5s target\"\n    \n    def test_batch_contract_processing(self, performance_test_data):\n        \"\"\"Benchmark batch processing of multiple contracts.\"\"\"\n        parser = ContractParser()\n        prover = LegalProver()\n        regulation = GDPR()\n        \n        contracts = performance_test_data['medium_contracts']\n        parsed_contracts = [parser.parse(text, f\"batch_{i}\") for i, text in enumerate(contracts)]\n        \n        # Benchmark batch processing\n        start_time = time.perf_counter()\n        \n        all_results = []\n        for contract in parsed_contracts:\n            results = prover.verify_compliance(contract, regulation)\n            all_results.append(results)\n        \n        end_time = time.perf_counter()\n        \n        total_time = end_time - start_time\n        avg_time_per_contract = total_time / len(contracts)\n        \n        print(f\"\\nBatch Processing Performance:\")\n        print(f\"Total contracts: {len(contracts)}\")\n        print(f\"Total time: {total_time:.3f}s\")\n        print(f\"Average time per contract: {avg_time_per_contract:.3f}s\")\n        print(f\"Throughput: {len(contracts) / total_time:.2f} contracts/second\")\n        \n        # Performance targets\n        assert avg_time_per_contract < 3.0, f\"Average time per contract {avg_time_per_contract:.3f}s exceeds target\"\n        assert len(contracts) / total_time > 2.0, f\"Throughput {len(contracts) / total_time:.2f} contracts/s below target\"\n        \n        # Verify all contracts were processed\n        assert len(all_results) == len(contracts)\n        for results in all_results:\n            assert len(results) > 0\n    \n    @pytest.mark.asyncio\n    async def test_concurrent_verification_performance(self, sample_contract_text):\n        \"\"\"Benchmark concurrent verification performance.\"\"\"\n        enhanced_prover = EnhancedLegalProver(max_workers=4)\n        parser = ContractParser()\n        regulation = GDPR()\n        \n        # Create multiple contract variants\n        contracts = []\n        for i in range(20):\n            contract_text = sample_contract_text.replace(\"Company A\", f\"Company A{i}\")\n            contract = parser.parse(contract_text, f\"concurrent_{i}\")\n            contracts.append(contract)\n        \n        # Sequential processing baseline\n        start_time = time.perf_counter()\n        sequential_results = []\n        for contract in contracts[:5]:  # Use fewer for sequential baseline\n            result = enhanced_prover.verify_compliance(contract, regulation)\n            sequential_results.append(result)\n        sequential_time = time.perf_counter() - start_time\n        \n        # Concurrent processing\n        start_time = time.perf_counter()\n        \n        concurrent_tasks = []\n        for contract in contracts:\n            task = enhanced_prover.verify_compliance_async(\n                contract=contract,\n                regulation=regulation\n            )\n            concurrent_tasks.append(task)\n        \n        concurrent_results = await asyncio.gather(*concurrent_tasks)\n        concurrent_time = time.perf_counter() - start_time\n        \n        # Calculate metrics\n        sequential_rate = len(sequential_results) / sequential_time\n        concurrent_rate = len(concurrent_results) / concurrent_time\n        speedup = concurrent_rate / sequential_rate\n        \n        print(f\"\\nConcurrent Verification Performance:\")\n        print(f\"Sequential rate: {sequential_rate:.2f} contracts/second\")\n        print(f\"Concurrent rate: {concurrent_rate:.2f} contracts/second\")\n        print(f\"Speedup factor: {speedup:.2f}x\")\n        print(f\"Concurrent processing time: {concurrent_time:.3f}s\")\n        \n        # Performance targets\n        assert concurrent_rate > sequential_rate, \"Concurrent processing should be faster\"\n        assert speedup > 1.5, f\"Speedup factor {speedup:.2f}x below 1.5x target\"\n        \n        # Verify all results\n        assert len(concurrent_results) == len(contracts)\n        for result in concurrent_results:\n            assert len(result) > 0\n    \n    def test_memory_efficiency_benchmark(self, sample_contract_text):\n        \"\"\"Benchmark memory efficiency during processing.\"\"\"\n        parser = ContractParser()\n        prover = LegalProver()\n        regulation = GDPR()\n        \n        # Baseline memory\n        gc.collect()\n        baseline_memory = self.process.memory_info().rss\n        \n        # Process contracts and monitor memory\n        memory_samples = []\n        num_contracts = 50\n        \n        for i in range(num_contracts):\n            # Create contract variant\n            contract_text = sample_contract_text + f\"\\n\\nAddendum {i}: Additional terms and conditions.\"\n            contract = parser.parse(contract_text, f\"memory_test_{i}\")\n            \n            # Verify compliance\n            results = prover.verify_compliance(contract, regulation)\n            assert len(results) > 0\n            \n            # Sample memory usage\n            current_memory = self.process.memory_info().rss\n            memory_increase = current_memory - baseline_memory\n            memory_samples.append(memory_increase)\n            \n            # Periodic cleanup\n            if i % 10 == 0:\n                gc.collect()\n        \n        # Final cleanup and measurement\n        gc.collect()\n        final_memory = self.process.memory_info().rss\n        final_increase = final_memory - baseline_memory\n        \n        # Calculate memory metrics\n        max_memory_increase = max(memory_samples)\n        avg_memory_increase = statistics.mean(memory_samples)\n        \n        print(f\"\\nMemory Efficiency Benchmark:\")\n        print(f\"Contracts processed: {num_contracts}\")\n        print(f\"Baseline memory: {baseline_memory / 1024 / 1024:.1f} MB\")\n        print(f\"Max memory increase: {max_memory_increase / 1024 / 1024:.1f} MB\")\n        print(f\"Average memory increase: {avg_memory_increase / 1024 / 1024:.1f} MB\")\n        print(f\"Final memory increase: {final_increase / 1024 / 1024:.1f} MB\")\n        print(f\"Memory per contract: {avg_memory_increase / num_contracts / 1024:.1f} KB\")\n        \n        # Memory efficiency targets\n        max_memory_mb = max_memory_increase / 1024 / 1024\n        avg_memory_mb = avg_memory_increase / 1024 / 1024\n        \n        assert max_memory_mb < 500, f\"Max memory increase {max_memory_mb:.1f}MB exceeds 500MB target\"\n        assert avg_memory_mb < 200, f\"Average memory increase {avg_memory_mb:.1f}MB exceeds 200MB target\"\n    \n    def test_cache_performance_benchmark(self, sample_contract_text):\n        \"\"\"Benchmark caching system performance.\"\"\"\n        cache_manager = CacheManager(l1_max_size=1000, l2_max_size=5000)\n        parser = ContractParser()\n        regulation = GDPR()\n        \n        # Parse contract\n        contract = parser.parse(sample_contract_text, \"cache_benchmark\")\n        \n        # Benchmark cache operations\n        num_operations = 1000\n        \n        # Cache set operations\n        set_times = []\n        for i in range(num_operations):\n            key = f\"test_key_{i}\"\n            value = {\"contract_id\": contract.id, \"data\": f\"test_data_{i}\", \"index\": i}\n            \n            start_time = time.perf_counter()\n            asyncio.run(cache_manager.set(key, value, ttl=300))\n            end_time = time.perf_counter()\n            \n            set_times.append(end_time - start_time)\n        \n        # Cache get operations (should hit cache)\n        get_times = []\n        for i in range(num_operations):\n            key = f\"test_key_{i}\"\n            \n            start_time = time.perf_counter()\n            value = asyncio.run(cache_manager.get(key))\n            end_time = time.perf_counter()\n            \n            get_times.append(end_time - start_time)\n            assert value is not None, f\"Cache miss for key {key}\"\n        \n        # Performance metrics\n        avg_set_time = statistics.mean(set_times)\n        avg_get_time = statistics.mean(get_times)\n        set_throughput = 1.0 / avg_set_time\n        get_throughput = 1.0 / avg_get_time\n        \n        cache_stats = cache_manager.get_stats()\n        \n        print(f\"\\nCache Performance Benchmark:\")\n        print(f\"Operations: {num_operations}\")\n        print(f\"Average set time: {avg_set_time*1000:.3f}ms\")\n        print(f\"Average get time: {avg_get_time*1000:.3f}ms\")\n        print(f\"Set throughput: {set_throughput:.0f} ops/second\")\n        print(f\"Get throughput: {get_throughput:.0f} ops/second\")\n        print(f\"L1 hit rate: {cache_stats['l1_memory'].hit_rate:.1f}%\")\n        \n        # Performance targets\n        assert avg_set_time < 0.001, f\"Average set time {avg_set_time*1000:.3f}ms exceeds 1ms target\"\n        assert avg_get_time < 0.0005, f\"Average get time {avg_get_time*1000:.3f}ms exceeds 0.5ms target\"\n        assert cache_stats['l1_memory'].hit_rate > 95, \"Cache hit rate below 95%\"\n    \n    @pytest.mark.slow\n    def test_stress_test_verification(self, performance_test_data):\n        \"\"\"Stress test with large number of contracts.\"\"\"\n        enhanced_prover = EnhancedLegalProver(max_workers=8, cache_enabled=True)\n        parser = ContractParser()\n        regulations = [GDPR(), AIAct(), CCPA()]\n        \n        # Use large contracts for stress test\n        large_contracts = performance_test_data['large_contracts'] * 10  # Multiply for more stress\n        \n        start_time = time.perf_counter()\n        start_memory = self.process.memory_info().rss\n        \n        processed_count = 0\n        total_results = 0\n        \n        try:\n            with ThreadPoolExecutor(max_workers=4) as executor:\n                futures = []\n                \n                for i, contract_text in enumerate(large_contracts):\n                    contract = parser.parse(contract_text, f\"stress_{i}\")\n                    \n                    for regulation in regulations:\n                        future = executor.submit(\n                            enhanced_prover.verify_compliance,\n                            contract,\n                            regulation\n                        )\n                        futures.append(future)\n                \n                # Process results as they complete\n                for future in as_completed(futures, timeout=300):  # 5 minute timeout\n                    try:\n                        results = future.result()\n                        total_results += len(results)\n                        processed_count += 1\n                        \n                        # Log progress\n                        if processed_count % 50 == 0:\n                            elapsed = time.perf_counter() - start_time\n                            rate = processed_count / elapsed\n                            print(f\"Processed {processed_count}/{len(futures)} ({rate:.2f}/s)\")\n                            \n                    except Exception as e:\n                        print(f\"Task failed: {e}\")\n        \n        except Exception as e:\n            print(f\"Stress test interrupted: {e}\")\n        \n        end_time = time.perf_counter()\n        end_memory = self.process.memory_info().rss\n        \n        total_time = end_time - start_time\n        memory_increase = end_memory - start_memory\n        \n        print(f\"\\nStress Test Results:\")\n        print(f\"Total contracts processed: {processed_count}\")\n        print(f\"Total verification results: {total_results}\")\n        print(f\"Total time: {total_time:.3f}s\")\n        print(f\"Processing rate: {processed_count / total_time:.2f} verifications/second\")\n        print(f\"Memory increase: {memory_increase / 1024 / 1024:.1f} MB\")\n        \n        # Stress test targets (more lenient than unit tests)\n        assert processed_count > len(futures) * 0.8, \"Should complete at least 80% of stress test tasks\"\n        assert total_time < 600, \"Stress test should complete within 10 minutes\"\n        assert memory_increase < 1024 * 1024 * 1024, \"Memory increase should be less than 1GB\"\n    \n    def test_regulation_complexity_benchmark(self):\n        \"\"\"Benchmark verification performance across different regulation complexities.\"\"\"\n        parser = ContractParser()\n        prover = LegalProver()\n        \n        # Create test contract\n        contract_text = \"\"\"\n        COMPREHENSIVE SERVICE AGREEMENT\n        \n        This agreement covers data processing, AI system deployment, and consumer privacy.\n        \n        1. DATA PROCESSING: Personal data will be processed securely with encryption.\n        2. AI TRANSPARENCY: AI systems will provide explanations for automated decisions.\n        3. CONSUMER RIGHTS: Users have rights to access, delete, and port their data.\n        4. SECURITY MEASURES: Multi-layered security controls protect all information.\n        5. HUMAN OVERSIGHT: Human reviewers monitor all AI system outputs.\n        6. BREACH NOTIFICATION: Security incidents reported within 72 hours.\n        7. DATA RETENTION: Data deleted after business purpose fulfilled.\n        8. CROSS-BORDER TRANSFERS: International transfers use adequate safeguards.\n        \"\"\"\n        \n        contract = parser.parse(contract_text, \"complexity_benchmark\")\n        \n        regulations = {\n            \"GDPR\": GDPR(),\n            \"AI Act\": AIAct(),\n            \"CCPA\": CCPA()\n        }\n        \n        results = {}\n        \n        for reg_name, regulation in regulations.items():\n            # Benchmark each regulation\n            times = []\n            \n            for _ in range(5):  # Multiple runs for accuracy\n                start_time = time.perf_counter()\n                verification_results = prover.verify_compliance(contract, regulation)\n                end_time = time.perf_counter()\n                \n                times.append(end_time - start_time)\n            \n            avg_time = statistics.mean(times)\n            requirement_count = len(regulation.get_requirements())\n            time_per_requirement = avg_time / requirement_count if requirement_count > 0 else 0\n            \n            results[reg_name] = {\n                'avg_time': avg_time,\n                'requirement_count': requirement_count,\n                'time_per_requirement': time_per_requirement,\n                'verification_results': len(verification_results)\n            }\n        \n        print(f\"\\nRegulation Complexity Benchmark:\")\n        for reg_name, metrics in results.items():\n            print(f\"{reg_name}:\")\n            print(f\"  Requirements: {metrics['requirement_count']}\")\n            print(f\"  Average time: {metrics['avg_time']:.3f}s\")\n            print(f\"  Time per requirement: {metrics['time_per_requirement']*1000:.1f}ms\")\n            print(f\"  Verification results: {metrics['verification_results']}\")\n        \n        # Performance assertions\n        for reg_name, metrics in results.items():\n            assert metrics['avg_time'] < 5.0, f\"{reg_name} verification time exceeds 5s\"\n            assert metrics['time_per_requirement'] < 0.2, f\"{reg_name} per-requirement time exceeds 200ms\"\n    \n    def test_worker_pool_performance(self):\n        \"\"\"Benchmark worker pool performance.\"\"\"\n        worker_pool = WorkerPool(cpu_workers=4, io_workers=8)\n        \n        # CPU-bound task\n        def cpu_task(n):\n            result = 0\n            for i in range(n):\n                result += i ** 2\n            return result\n        \n        # I/O-bound task (simulate)\n        def io_task(delay):\n            time.sleep(delay)\n            return delay\n        \n        # Benchmark CPU tasks\n        cpu_start = time.perf_counter()\n        cpu_futures = []\n        for i in range(20):\n            future = asyncio.run(worker_pool.submit_cpu_task(cpu_task, 10000))\n            cpu_futures.append(future)\n        \n        cpu_end = time.perf_counter()\n        cpu_time = cpu_end - cpu_start\n        \n        # Benchmark I/O tasks\n        io_start = time.perf_counter()\n        io_futures = []\n        for i in range(20):\n            future = asyncio.run(worker_pool.submit_io_task(io_task, 0.1))\n            io_futures.append(future)\n        \n        io_end = time.perf_counter()\n        io_time = io_end - io_start\n        \n        stats = worker_pool.get_stats()\n        \n        print(f\"\\nWorker Pool Performance:\")\n        print(f\"CPU tasks (20): {cpu_time:.3f}s\")\n        print(f\"I/O tasks (20): {io_time:.3f}s\")\n        print(f\"CPU workers: {stats['cpu_workers']}\")\n        print(f\"I/O workers: {stats['io_workers']}\")\n        print(f\"Completed tasks: {stats['completed_tasks']}\")\n        print(f\"Failed tasks: {stats['failed_tasks']}\")\n        \n        # Performance targets\n        assert cpu_time < 10.0, f\"CPU task batch time {cpu_time:.3f}s exceeds 10s target\"\n        assert io_time < 5.0, f\"I/O task batch time {io_time:.3f}s exceeds 5s target\"\n        assert stats['completed_tasks'] >= 40, \"Should complete all submitted tasks\"\n        assert stats['failed_tasks'] == 0, \"Should have no failed tasks\"\n        \n        worker_pool.shutdown()\n    \n    def save_benchmark_results(self, results: Dict[str, Any], output_file: str):\n        \"\"\"Save benchmark results to file.\"\"\"\n        output_path = Path(output_file)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        with open(output_path, 'w') as f:\n            json.dump(results, f, indent=2, default=str)\n        \n        print(f\"Benchmark results saved to {output_path}\")"